"""This script is to generate the HH dataset for SFT.

The generated dataset will be saved in the folder `data` with the following
structure:
        data
        └── helpful-base
            ├── train.json
            └── test.json

The `train.json` and `test.json` are the training and testing dataset, of which
the format is as follows:
    [
        ...
        {
            "prompt": str, the prompt,
            "chosen": str, the chosen response,
            "rejected": str, the rejected response,
            "paraphrase": str, a paraphrase of the rejected response,
            "variant": str, a response generated by a different model,
            "random": str, the chosen response to another prompt randmly
                      seleted from the dataset,
            "nonresponse": str, a sentence that is not a response to the prompt
            # so far, the non-response is randomly generated by GPT2-medium
        }
        ...
    ]
"""
import argparse
import hashlib
import json
import random
import requests
import time
from pathlib import Path
from tqdm import trange
from typing import List, Dict

import datasets
import google.generativeai as genai
from openai import OpenAI
from preference_datasets import extract_anthropic_prompt


KEY_IDX = 0
KEY_LIST = None
PLATFORM = 'baichuan'
MODEL = 'Baichuan2-53B'
PLATFORM2BASEURL = {
    'openai': 'https://api.openai.com',
    'baichuan': 'https://api.baichuan-ai.com/v1/chat/completions',
}


def get_original_helpful(split: str = None):
    """Get the original HH dataset.
    
    Args:
        split: the split of the dataset to be loaded.
    
    Returns:
        dataset: the original HH dataset.
    """
    print(f'Loading HH dataset (helpful-base split) from Huggingface...')
    if split is None:
        dataset = datasets.load_dataset(
            'Anthropic/hh-rlhf', data_dir='helpful-base', cache_dir='./data'
        )
    else:
        dataset = datasets.load_dataset(
            'Anthropic/hh-rlhf', data_dir='helpful-base', cache_dir='./data',
            split=split
        )
    print('done')
    return dataset


def get_api_base_url(platform:str='openai') -> str:
    """Get the base URL of the API.

    Returns:
        base_url: the base URL of the API.
    """
    base_url = None
    file_path = Path('api_base_url.txt')
    if file_path.exists():
        with open(file_path, 'r') as f:
            base_url = f.read().strip()
    else:
        base_url = PLATFORM2BASEURL[platform]
    return base_url


def get_api_key() -> str:
    """Get the list of API keys.

    Returns:
        api_key_list: the list of API keys.
    """
    global KEY_IDX, KEY_LIST
    if KEY_LIST is None:
        with open('api_keys.txt', 'r') as f:
            KEY_LIST = [line.strip() for line in f.readlines()]
    api_key = KEY_LIST[KEY_IDX % len(KEY_LIST)]
    KEY_IDX += 1
    return api_key


def get_secret_key() -> str:
    """Get the list of API secret keys.

    Returns:
        api_key_list: the list of API secret keys.
    """
    global KEY_IDX, KEY_LIST
    if KEY_LIST is None:
        with open('api_sceret_keys.txt', 'r') as f:
            KEY_LIST = [line.strip() for line in f.readlines()]
    api_key = KEY_LIST[KEY_IDX % len(KEY_LIST)]
    KEY_IDX += 1
    return api_key


def get_gpt_completion(prompt: str) -> str:
    """Get the GPT-3 completion of the prompt.

    Args:
        prompt: the prompt.

    Returns:
        completion: the GPT-3 completion of the prompt.
    """
    client = OpenAI(
        api_key=get_api_key(),
        base_url=get_api_base_url('openai'),
    )
    completion = client.chat.completions.create(
        model=MODEL,
        messages=[
            {'role': 'user', 'content': prompt},
        ],
        temperature=0.0,
        n=1,
    )
    return completion.choices[0].message.content


def get_gemini_completion(prompt: str) -> str:
    """Get the Gemini completion of the prompt.

    Args:
        prompt: the prompt.

    Returns:
        completion: the Gemini completion of the prompt.
    """
    genai.configure(api_key=get_api_key())
    model = genai.GenerativeModel('gemini-pro')
    completion = model.generate_content(prompt)
    return completion.text


def get_baichuan_completion(prompt: str) -> str:
    """Get the Baichuan completion of the prompt.

    Args:
        prompt: the prompt.

    Returns:
        completion: the Baichuan completion of the prompt.
    """
    api_key = get_api_key()
    base_ulr = get_api_base_url('baichuan')
    secret_key = get_secret_key()

    data = {
        'model': MODEL,
        'messages': [
            {'role': 'user', 'content': prompt},
        ],
        'parameters': {
            'temperature': float(0)
        }
    }
    json_data = json.dumps(data)
    time_stamp = int(time.time())
    md5 = hashlib.md5()
    md5.update((secret_key + json_data + str(time_stamp)).encode('utf-8'))
    signature = md5.hexdigest()
    headers = {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer ' + api_key,
    }

    while True:
        response = requests.post(
            base_ulr, data=json_data, headers=headers, timeout=60
        )
        if response.status_code == 200:
            break
    response = response.json()
    return response['choices'][0]['message']['content']


def get_completion(prompt: str, platform: str = 'gemini') -> str:
    """Get the completion of the prompt.

    Args:
        prompt: the prompt.

    Returns:
        completion: the completion of the prompt.
    """
    if platform == 'openai':
        return get_gpt_completion(prompt)
    elif platform == 'gemini':
        return get_gemini_completion(prompt)
    elif platform == 'baichuan':
        return get_baichuan_completion(prompt)


def get_paraphrase(rejected_response: str) -> str:
    """Get a paraphrase of the rejected response.

    Args:
        rejected_response: the rejected response.

    Returns:
        paraphrase: a paraphrase of the rejected response.
    """
    gpt_prompt = f'Paraphrase the following sentence within 50 words:\n"{rejected_response}"\nParaphrase:'
    return get_completion(gpt_prompt, PLATFORM)


def get_variant_response(prompt: str) -> str:
    """Get a variant response to the prompt.

    Args:
        prompt: the prompt.

    Returns:
        response: a variant response generated by .
    """
    gpt_prompt = f'Complete the following conversation within 50 words:\n{prompt}'
    return get_completion(gpt_prompt, PLATFORM)


def get_nonreponse(prompt: str) -> str:
    """Get a non-response to the prompt.

    Args:
        prompt: the prompt.

    Returns:
        nonresponse: a non-response.
    """
    gpt_prompt = f'Give me a random sentence.'
    return get_completion(gpt_prompt, PLATFORM)


def convert_helpful_base(dataset: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """Convert the HH dataset to the format described above.

    Args:
        dataset: the original HH helpful-base subset.

    Returns:
        converted_dataset: the converted dataset.
    """
    converted_dataset = []
    for idx in trange(0, len(dataset) -1):
        ex = dataset[idx]
        prompt = extract_anthropic_prompt(ex['chosen'])
        chosen_response = ex['chosen'][len(prompt):]
        rejected_response = ex['rejected'][len(prompt):]
        variant_response = get_variant_response(prompt)
        non_response =  get_nonreponse(prompt)
        paraphrase = get_paraphrase(chosen_response)

        while True:
            random_idx = random.randint(0, len(dataset) - 1)
            if random_idx != idx:
                break
        random_prompt = extract_anthropic_prompt(dataset[random_idx]['chosen'])
        random_response = dataset[random_idx]['chosen'][len(random_prompt):]

        converted_dataset.append({
            'prompt': prompt,
            'chosen': chosen_response,
            'rejected': rejected_response,
            'random': random_response,
            'variant': variant_response,
            'paraphrase': paraphrase,
            'nonresponse': non_response
        })
    return converted_dataset


def main(
    split:str = 'test',
    start_frac : int = 0,
    end_frac: int = 10
) -> None:
    if split == 'test':
        data_split = 'test'
        helpful_dataset = get_original_helpful(data_split)
        print('Converting the test set...')
        converted_test = convert_helpful_base(helpful_dataset)
        with open('data/helpful-base/test.json', 'w') as f:
            f.write(json.dumps(converted_test))
        print('done')
    else:
        data_split = f'train[{start_frac}%:{end_frac}%]'
        helpful_dataset = get_original_helpful(data_split)
        print(
            f'Converting the training set from {start_frac}% to {end_frac}%...'
        )
        converted_train = convert_helpful_base(helpful_dataset)
        with open(
            f'data/helpful-base/train_{start_frac}%_{end_frac}%.json', 'w'
        ) as f:
            f.write(json.dumps(converted_train))
        print('done')


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Process a range of indices.")
    parser.add_argument('--start_frac', type=int, default=0,
        help='Starting index'
    )
    parser.add_argument('--end_frac', type=int, default=10,
        help='Ending index'
    )
    parser.add_argument('--split', type=str, default='test',
        help='The split of the dataset to be converted'
    )
    args = parser.parse_args()

    main(args.split, args.start_frac, args.end_frac)
